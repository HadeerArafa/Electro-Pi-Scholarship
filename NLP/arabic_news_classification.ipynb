{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#data_loading\">data_loading</a></li>\n",
    "<li><a href=\"#text_encoding\">text_encoding</a></li>\n",
    "<li><a href=\"#text_cleaning\">text_cleaning</a></li> \n",
    "        - remove numbers<br>\n",
    "        -  remove stop words<br>\n",
    "        - remove english word<br>\n",
    "        - remove sepcail chars<br>\n",
    "<li><a href=\"#data_exploration\">data_exploration</a></li>\n",
    "<li><a href=\"#bag_of_words\">bag_of_words</a></li>\n",
    "        - svm model<br>\n",
    "        - bag_of_words_prediction_function<br>\n",
    "<li><a href=\"#word_vectors\">word_vectors</a></li>\n",
    "        - model_loading<br>\n",
    "        - spacy<br>\n",
    "        - preprocessing<br>\n",
    "        - svm<br>\n",
    "        - aravec_prediction_function<br>\n",
    "<li><a href=\"#deep_learning\">deep_learning</a></li>\n",
    "        - get embeddings matrix<br>\n",
    "        - preprocessing<br>\n",
    "        - LSTM model<br>\n",
    "        - LSTM_prediction_function<br>\n",
    "<li><a href=\"#CaMel_BERT\">CaMel_BERT</a></li>\n",
    "        - model_loading<br>\n",
    "        - data_tokanization<br>\n",
    "        - data_preperation<br>\n",
    "        - fine_tune_model<br>\n",
    "        - model_training<br>\n",
    "        - test_preperation_function<br>\n",
    "        - CaMel_prediction_function<br>\n",
    "<li><a href=\"#AraBert\">AraBert</a></li>\n",
    "        - model_loading<br>\n",
    "        - data_tokanization<br>\n",
    "        - data_preperation<br>\n",
    "        - fine_tune_model<br>\n",
    "        - model_training<br>\n",
    "        - test_preperation_function<br>\n",
    "        - AraBert_prediction_function<br>\n",
    "<li><a href=\"#BERT_Multilingual\">BERT_Multilingual</a></li>\n",
    "        - model_loading<br>\n",
    "        - data_tokanization<br>\n",
    "        - data_preperation<br>\n",
    "        - fine_tune_model<br>\n",
    "        - model_training<br>\n",
    "        - test_preperation_function<br>\n",
    "        - AraBert_prediction_function<br>\n",
    "<li><a href=\"#comparison\">comparison</a></li>\n",
    "- Number of Epochs <br>\n",
    "- Run Time <br>\n",
    "- Train Acc <br>\n",
    "</ul>\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import Embedding, Dense, Dropout, Input, LSTM, GlobalMaxPool1D , TextVectorization\n",
    "import spacy\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk import word_tokenize # break a sentence into a words \n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from collections import Counter #counting of words in the texts\n",
    "import operator\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"D:/Bi electric scolership/NLP/data_cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = os.listdir(directory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Culture', 'Finance', 'Medical', 'Politics', 'Religion', 'Sports', 'Tech']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45500 files belonging to 7 classes.\n",
      "Using 31850 files for training.\n",
      "Found 45500 files belonging to 7 classes.\n",
      "Using 13650 files for validation.\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "train_data =  tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    directory = directory,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=labels,\n",
    "    max_length=None,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.3,\n",
    "    subset=\"training\",\n",
    "    follow_links=False,\n",
    ")\n",
    "val_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    directory = directory,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=labels,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    follow_links=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xd8\\xb1\\xd8\\xa3\\xd8\\xb3 \\xd8\\xa7\\xd9\\x84\\xd8\\xae\\xd9\\x8a\\xd9\\x85\\xd8\\xa9 - \\xd8\\xb9\\xd8\\xaf\\xd9\\x86\\xd8\\xa7'\n"
     ]
    }
   ],
   "source": [
    "for text , label in train_data:\n",
    "    print(text[0].numpy()[0:30])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> just will output just sampels because we still need to decode our text because they are arabic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2numpy(data):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for text_batch, label_batch in data:\n",
    "        for i in range(len(text_batch)):\n",
    "            s=text_batch.numpy()[i].decode(\"utf-8\") \n",
    "            X.append(s)\n",
    "            y.append(data.class_names[label_batch.numpy()[i]])\n",
    "    return X , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , y_train = data2numpy(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val , y_val = data2numpy(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'تعتزم دائرة البلدية والتخطيط في عجمان عرض آخر خدماتها التقنية التي توفرها لجمهور المتعاملين معها إلك'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> before these whould just print sampels but because we encoded the data , our data is now right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after seeing the example above we can see some issues :\n",
    "- there are numbers that will add nothing to the text meaning\n",
    "- there are stop words that wont also add anything to the text meaning \n",
    "- there are english words that we should get rid of\n",
    "- and finaly to remove special chars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['مثل', 'أمد', 'شيكل', 'أما', 'ثم', 'حمٌ', 'تاء', 'هيّا', 'هاتي', 'ثلاث', 'ذ', 'ألف', 'عدا', 'ذين', 'بعض', 'لم', 'طفق', 'استحال', 'كِخ', 'لست']\n"
     ]
    }
   ],
   "source": [
    "stop_words = list(set(stopwords.words('arabic')))\n",
    "print(stop_words[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('arabic'))\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_diacritics(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    text = ' '.join([word for word in tokens if word not in stop_words])\n",
    "    text = re.sub(\"\\d+\", \"\", text) # to remove digits\n",
    "    text = re.sub(r'\\s*[A-Za-z]+\\b', '' , text) # to remove english words\n",
    "    text = text.rstrip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cleaned = [clean_text(text) for text in X_train]\n",
    "X_val_cleaned = [clean_text(text) for text in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_text : \n",
      "تعتزم دائرة البلدية والتخطيط في عجمان عرض آخر خدماتها التقنية التي توفرها لجمهور المتعاملين معها إلكترونياً خلال مشاركتها بمعرض جيتكس 2009 الذي يفتتح اليوم الأحد في مركز دبي التجاري العالمي، ووجهت الدائرة دعوة مفتوحة لكافة المهتمين بالمعرض وللمؤسسات الإعلامية لزيارة جناحها رقم 908 في قاعة الشيخ راشد بالمركز من خلال إدارة تقنية المعلومات بالدائرة وقسم نظم المعلومات الجغرافية GIS .أكد راشد الكعبي رئ\n",
      "===================================================================\n",
      "text after cleaning : \n",
      "تعتزم دائرة البلدية والتخطيط عجمان عرض آخر خدماتها التقنية توفرها لجمهور المتعاملين معها إلكترونيا خلال مشاركتها بمعرض جيتكس  يفتتح اليوم الأحد مركز دبي التجاري العالمي، ووجهت الدائرة دعوة مفتوحة لكافة المهتمين بالمعرض وللمؤسسات الإعلامية لزيارة جناحها رقم  قاعة الشيخ راشد بالمركز خلال إدارة تقنية المعلومات بالدائرة وقسم نظم المعلومات الجغرافية أكد راشد الكعبي رئيس قسم نظم المعلومات الجغرافية المع\n"
     ]
    }
   ],
   "source": [
    "print('original_text : ')\n",
    "print(X_train[0][0:400])\n",
    "print(\"===================================================================\")\n",
    "print('text after cleaning : ')\n",
    "print(X_train_cleaned[0][0:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_cleaned.copy()\n",
    "X_val= X_val_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# del X_train_cleaned\n",
    "# gc.collect()\n",
    "del X_val_cleaned\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> now let's see if the data was balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(y :list):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    plt.bar( unique,counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'val')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(1,2,1)\n",
    "plot_bar(y_train)\n",
    "plt.title('train')\n",
    "plt.subplot(1,2,2)\n",
    "plot_bar(y_val)\n",
    "plt.title('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> so our data is quite palanced in both `train` and `val`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_vectors = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Medical', 'Finance', 'Sports', 'Politics', 'Religion']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_val= label_encoder.transform(y_val)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm = svm.SVC(kernel='linear')\n",
    "clf_svm.fit(train_x_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_vectors= vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf_svm.predict(X_val_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.52 %\n"
     ]
    }
   ],
   "source": [
    "print( round(metrics.accuracy_score(y_val,res) *100  , 2) , \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_labels(text):\n",
    "    text_vectorized = vectorizer.transform(text)\n",
    "    res = clf_svm.predict(text_vectorized)\n",
    "    print(labels[res[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new class is :  Sports\n",
      "part of the news text :  تتجه أنظار الملايين من عشاق الساحرة المستديرة في كل أنحاء الوطن العربي صوب العاصمتين البرازيليتين، الحالية برازيليا والقديمة ريو دي جانيرو لمتابعة ضربة البداية لمسيرة كل من المنتخبين العراقي والجزائري\n"
     ]
    }
   ],
   "source": [
    "print(\"new class is : \" , end = ' ')\n",
    "news_labels([X_val[2]])\n",
    "print(\"part of the news text : \" , end = ' ')\n",
    "print(X_val[2][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the limitation of this approtch is that is there is a new word in test set the model will not be able to handel it and just give it a `0` and this word may be an altternitive in word in training and the model will still not be able to get it \n",
    "- we need more than just representing the sentence like that we need word `embedding` where words that has similary meaning are close to each other in the `vector space`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word vectors\n",
    "- `word2vec`\n",
    "    - word2vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through word2vec have proven to be successful on a variety of downstream natural language processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> i got an arabic embedding model from the internet and i will use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_model_path = 'D:/Bi electric scolership/NLP/arabic_model'\n",
    "model_name = 'full_grams_cbow_100_twitter.mdl'\n",
    "model = gensim.models.Word2Vec.load(os.path.join(arabic_model_path , model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've 1476715 vocabularies\n"
     ]
    }
   ],
   "source": [
    "print(\"We've\",len(model.wv.index_to_key),\"vocabularies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(\"D:/Bi electric scolership/NLP/spacyModel/aravec.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ℹ Creating blank nlp object for language 'ar'\n",
      "✔ Successfully converted 1476715 vectors\n",
      "✔ Saved nlp object with vectors to output directory. You can now use the path to\n",
      "it in your config as the 'vectors' setting in [initialize].\n",
      " \n"
     ]
    }
   ],
   "source": [
    "!python -m spacy  init vectors ar \"D:/Bi electric scolership/NLP/spacyModel/aravec.zip\" \"D:/Bi electric scolership/NLP/spacy.aravec.model\" \n",
    "# returns a spaCy pipeline directory containing the vocab and vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"D:/Bi electric scolership/NLP/spacy.aravec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, tokenizer, **cfg):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, text):\n",
    "        preprocessed = clean_str(text)\n",
    "        return self.tokenizer(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer = Preprocessor(nlp.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(text) for text in X_train]\n",
    "train_x_word_vectors = [x.vector for x in docs]\n",
    "\n",
    "val_docs = [nlp(text) for text in X_val]\n",
    "val_x_word_vectors =  [x.vector for x in val_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs))\n",
    "print(type(docs[0]))\n",
    "print(type(docs[0].vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we can see that `docs` is a list and each element in it is a emdedding vector for each training example\n",
    "- so we need to use `.vector` to convert them to array so we can use them in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_wv = svm.SVC(kernel='linear')\n",
    "clf_svm_wv.fit(train_x_word_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_word_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf_svm_wv.predict(val_x_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.03 %\n"
     ]
    }
   ],
   "source": [
    "print( round(metrics.accuracy_score(y_val,res) *100  , 2) , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aravec_predict(text):\n",
    "    test_docs = [nlp(text) for text in text]\n",
    "    test_x_word_vectors =  [x.vector for x in test_docs]\n",
    "    res = clf_svm_wv.predict(test_x_word_vectors)\n",
    "    print(labels[res[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new class is :  Medical\n",
      "part of the news text :  تزامناً مع يوم العمال العالمي، الذي يحتفل به العالم في الأول من مايو من كل عام، نظم سوق الجبيل، يومي أمس  واليوم  إبريل الجاري، فحوصاً طبية مجانية للمستثمرين في السوق، والعمال، والمتسوقين، والزوار\n"
     ]
    }
   ],
   "source": [
    "sample_index = 150\n",
    "print(\"new class is : \" , end = ' ')\n",
    "aravec_predict([X_val[sample_index]])\n",
    "print(\"part of the news text : \" , end = ' ')\n",
    "print(X_val[sample_index][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> now there are some improvments\n",
    "- first thing is that : now even if there is a word that was not in training and in val only , the model will still be able to classify it as long as this word is in our dict\n",
    "- new alternative words in val and not in train will still get a good classification as long it's on our dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> one thing about word embeddings is that :\n",
    "- it will give the same embedding for the same word in different sentence even if there have different context in these diiferent sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> now from the pretraining embeddings i want to get to get an embeddings matrix for each word in my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "vocab_size = 100000\n",
    "oov_token = \"<OOV>\"\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "padding_type='post'\n",
    "truncation_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_sequences,maxlen=max_length, padding=padding_type, \n",
    "                       truncating=truncation_type)\n",
    "\n",
    "X_test_padded = pad_sequences(X_val_sequences,maxlen=max_length, \n",
    "                               padding=padding_type, truncating=truncation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, max_length))\n",
    "for word, i in word_index.items():\n",
    "    if i ==1: # out of vocab token\n",
    "        pass\n",
    "    else : \n",
    "        embedding_vector = nlp(word).vector\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional , Dropout\n",
    "\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1,\n",
    "                            output_dim=max_length,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "classes = len(labels)\n",
    "model_v2 = Sequential([\n",
    "    embedding_layer,\n",
    "    Bidirectional(LSTM(150, return_sequences=True)), \n",
    "    Bidirectional(LSTM(150)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu') , \n",
    "    Dropout(0.2),\n",
    "   Dense(classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v2.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 100, 100)          56558800  \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 100, 300)         301200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 300)              541200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 128)               38528     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,448,439\n",
      "Trainable params: 889,639\n",
      "Non-trainable params: 56,558,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1991/1991 [==============================] - 540s 337ms/step - loss: 1.7798 - accuracy: 0.2291 - val_loss: 1.7237 - val_accuracy: 0.3461\n",
      "Epoch 2/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 1.7081 - accuracy: 0.2875 - val_loss: 1.5634 - val_accuracy: 0.5663\n",
      "Epoch 3/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 1.5288 - accuracy: 0.3914 - val_loss: 1.2559 - val_accuracy: 0.6225\n",
      "Epoch 4/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 1.3055 - accuracy: 0.4916 - val_loss: 0.9509 - val_accuracy: 0.8247\n",
      "Epoch 5/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 1.0466 - accuracy: 0.5971 - val_loss: 0.7033 - val_accuracy: 0.8674\n",
      "Epoch 6/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.8558 - accuracy: 0.6749 - val_loss: 0.5356 - val_accuracy: 0.8989\n",
      "Epoch 7/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.7154 - accuracy: 0.7374 - val_loss: 0.4175 - val_accuracy: 0.9146\n",
      "Epoch 8/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.5899 - accuracy: 0.8022 - val_loss: 0.3331 - val_accuracy: 0.9191\n",
      "Epoch 9/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.5168 - accuracy: 0.8155 - val_loss: 0.2753 - val_accuracy: 0.9303\n",
      "Epoch 10/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.4546 - accuracy: 0.8403 - val_loss: 0.2489 - val_accuracy: 0.9303\n",
      "Epoch 11/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.4120 - accuracy: 0.8528 - val_loss: 0.2138 - val_accuracy: 0.9393\n",
      "Epoch 12/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.3102 - accuracy: 0.8999 - val_loss: 0.1869 - val_accuracy: 0.9416\n",
      "Epoch 13/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.3185 - accuracy: 0.8917 - val_loss: 0.1848 - val_accuracy: 0.9371\n",
      "Epoch 14/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.2809 - accuracy: 0.9099 - val_loss: 0.1855 - val_accuracy: 0.9393\n",
      "Epoch 15/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.2728 - accuracy: 0.9099 - val_loss: 0.1717 - val_accuracy: 0.9438\n",
      "Epoch 16/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.2570 - accuracy: 0.9083 - val_loss: 0.1631 - val_accuracy: 0.9461\n",
      "Epoch 17/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1983 - accuracy: 0.9451 - val_loss: 0.1718 - val_accuracy: 0.9416\n",
      "Epoch 18/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.2196 - accuracy: 0.9369 - val_loss: 0.1724 - val_accuracy: 0.9416\n",
      "Epoch 19/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1843 - accuracy: 0.9438 - val_loss: 0.1719 - val_accuracy: 0.9416\n",
      "Epoch 20/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1728 - accuracy: 0.9437 - val_loss: 0.1688 - val_accuracy: 0.9438\n",
      "Epoch 21/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1745 - accuracy: 0.9319 - val_loss: 0.1654 - val_accuracy: 0.9438\n",
      "Epoch 22/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1523 - accuracy: 0.9514 - val_loss: 0.1675 - val_accuracy: 0.9438\n",
      "Epoch 23/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1729 - accuracy: 0.9417 - val_loss: 0.1719 - val_accuracy: 0.9438\n",
      "Epoch 24/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1323 - accuracy: 0.9532 - val_loss: 0.1821 - val_accuracy: 0.9416\n",
      "Epoch 25/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1469 - accuracy: 0.9522 - val_loss: 0.1854 - val_accuracy: 0.9416\n",
      "Epoch 26/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1329 - accuracy: 0.9562 - val_loss: 0.1875 - val_accuracy: 0.9461\n",
      "Epoch 27/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1026 - accuracy: 0.9727 - val_loss: 0.1960 - val_accuracy: 0.9483\n",
      "Epoch 28/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1217 - accuracy: 0.9540 - val_loss: 0.2017 - val_accuracy: 0.9438\n",
      "Epoch 29/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1318 - accuracy: 0.9487 - val_loss: 0.2101 - val_accuracy: 0.9461\n",
      "Epoch 30/30\n",
      "1991/1991 [==============================] - 364s 225ms/step - loss: 0.1191 - accuracy: 0.9542 - val_loss: 0.2133 - val_accuracy: 0.9461\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "history = model_v2.fit(X_train_padded, y_train, epochs=epochs, validation_data=(X_test_padded, y_val) , batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_predict(text):\n",
    "    test_sequences = tokenizer.texts_to_sequences(text)\n",
    "    test_sequences_padded = pad_sequences(test_sequences,maxlen=max_length, padding=padding_type, \n",
    "                        truncating=truncation_type)\n",
    "    probs = model_v2.predict(test_sequences_padded)\n",
    "    res = np.argmax(probs)\n",
    "    print(labels[res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class is :  Medical\n",
      "part of the news text :  تزامناً مع يوم العمال العالمي، الذي يحتفل به العالم في الأول من مايو من كل عام، نظم سوق الجبيل، يومي أمس  واليوم  إبريل الجاري، فحوصاً طبية مجانية للمستثمرين في السوق، والعمال، والمتسوقين، والزوار\n"
     ]
    }
   ],
   "source": [
    "sample_index = 150\n",
    "print(\"class is : \" , end = ' ')\n",
    "LSTM_predict(X_val[sample_index])\n",
    "print(\"part of the news text : \" , end = ' ')\n",
    "print(X_val[sample_index][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAMeL-Lab/bert : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> using `bert` to get embeddings and fine tune the model with additional layers for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hadee\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel , AutoModelForMaskedLM , TFAutoModel\n",
    "from transformers import TFAutoModelForSequenceClassification , TFTrainer, TFTrainingArguments\n",
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"CAMeL-Lab/bert-base-arabic-camelbert-da\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 86.0/86.0 [00:00<00:00, 30.7kB/s]\n",
      "Downloading config.json: 100%|██████████| 980/980 [00:00<00:00, 246kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 297k/297k [00:00<00:00, 380kB/s]  \n",
      "Downloading special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 42.4kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "Camel_model = TFAutoModel.from_pretrained('CAMeL-Lab/bert-base-arabic-camelbert-da')\n",
    "classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109081344 \n",
      "=================================================================\n",
      "Total params: 109,081,344\n",
      "Trainable params: 109,081,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Camel_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initializing empty arrays to store tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31850, 200)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200\n",
    "num_samples = len(X_train)\n",
    "\n",
    "Xids = np.zeros((num_samples, seq_len))\n",
    "Xmask = np.zeros((num_samples, seq_len))\n",
    "\n",
    "Xids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, phrase in enumerate(X_train):\n",
    "    tokens = tokenizer.encode_plus(phrase, max_length=seq_len, truncation=True,\n",
    "                                   padding='max_length', add_special_tokens=True,\n",
    "                                   return_tensors='tf')\n",
    "    Xids[i, :] = tokens['input_ids']\n",
    "    Xmask[i, :] = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "train_dataset = train_dataset.map(map_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataset = train_dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
    "\n",
    "\n",
    "embeddings = Camel_model.bert(input_ids, attention_mask=mask)[1]  # access pooled activations with [1]\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(classes, activation='softmax', name='outputs')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Camel_model_tuned = tf.keras.Model(inputs=[input_ids, mask], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 109081344   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         787456      bert[2][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 7)            7175        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 109,875,975\n",
      "Trainable params: 109,875,975\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Camel_model_tuned.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "Camel_model_tuned.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1991/1991 [==============================] - 814s 402ms/step - loss: 0.1339 - accuracy: 0.8432\n",
      "Epoch 2/5\n",
      "1991/1991 [==============================] - 799s 401ms/step - loss: 0.0669 - accuracy: 0.9218\n",
      "Epoch 3/5\n",
      "1991/1991 [==============================] - 799s 401ms/step - loss: 0.0490 - accuracy: 0.9272\n",
      "Epoch 4/5\n",
      "1991/1991 [==============================] - 800s 402ms/step - loss: 0.0500 - accuracy: 0.9251\n",
      "Epoch 5/5\n",
      "1991/1991 [==============================] - 799s 401ms/step - loss: 0.0347 - accuracy: 0.9424\n"
     ]
    }
   ],
   "source": [
    "history = Camel_model_tuned.fit(\n",
    "    train_dataset,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> needed  more epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X_train[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(1, 200), dtype=int32, numpy=\n",
      "array([[    2,  5365,  8311,    17, 14962, 12233,  3463,    30,  2136,\n",
      "          395,    18,  2195,  1953, 12426,  1013,   378,  3615,  3532,\n",
      "         5365,  8311,  5563,    30,  2259,  5482, 19524,  1912,  3680,\n",
      "         6997,  2046,  3231,  8736,  1017,  3647,  4472,  5919, 15287,\n",
      "          378,  2130,  4544,  1908, 14815, 12430,  5919,  1912,  4039,\n",
      "         2116,   378,  2455, 17768,  2268,   378,  6973,  1069,  6922,\n",
      "         7094,     9,   378,  2298,  9617, 12059,  1908,  8677,  1983,\n",
      "         3808,  3421,  1912,  5457,    19, 10881,  4656,    18, 16446,\n",
      "        19145,  5350, 29068,   378,  2130, 27261,  2854,  2366,     1,\n",
      "         2045,  2823, 12059,  1908,  3680, 13249,  2046,   378, 14474,\n",
      "         1010,  7276,  2045, 10212,     1,   378,  7627,  1955, 21370,\n",
      "        14939, 11114,    18,     1, 18985, 12426,  1013,   378,  4557,\n",
      "         3808,   378,  2130,  7772,  3808,  1912,  3532,  8736,  1017,\n",
      "          378,  3487,  4986, 22961,  4470,  3231,  3255,  5365,  8311,\n",
      "          378,  1908,  4462, 12430,  5919,  6953, 23107,  5365,  8311,\n",
      "         5563,   378,  3398,  2103,  1908, 12387, 15146, 13965,  1946,\n",
      "         1908,  3591,  2563,   378, 23306,  4063,  8216,  1043, 13869,\n",
      "         2600,    18,  5816,  2419,  1010,  3808,   378,  1912, 13848,\n",
      "         1028, 11161,   378,  1961,  6471,  3481,  2773,  7470, 19336,\n",
      "         7374,  7790,  5919,     6, 24554,     6, 28582, 15269,  2558,\n",
      "        14839, 19964, 25886,   378,  2813,  3328,     1,     1,   378,\n",
      "        18562,  2224,  4985,  3175,  1043, 25747,  1009,   411,  7094,\n",
      "         6596,     3]], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode_plus(sample, max_length=seq_len, truncation=True,\n",
    "                                   padding='max_length', add_special_tokens=True,\n",
    "                                   return_tensors='tf')\n",
    "tokens['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_Camel(text):\n",
    "    # tokenize to get input IDs and attention mask tensors\n",
    "    tokens = tokenizer.encode_plus(text, max_length=seq_len,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_token_type_ids=False,\n",
    "                                   return_tensors='tf')\n",
    "    return {'input_ids': tf.cast(tokens['input_ids'], tf.int32),\n",
    "            'attention_mask': tf.cast(tokens['attention_mask'], tf.int32)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 200), dtype=int32, numpy=\n",
      " array([[    2,  5365,  8311,    17, 14962, 12233,  3463,    30,  2136,\n",
      "           395,    18,  2195,  1953, 12426,  1013,   378,  3615,  3532,\n",
      "          5365,  8311,  5563,    30,  2259,  5482, 19524,  1912,  3680,\n",
      "          6997,  2046,  3231,  8736,  1017,  3647,  4472,  5919, 15287,\n",
      "           378,  2130,  4544,  1908, 14815, 12430,  5919,  1912,  4039,\n",
      "          2116,   378,  2455, 17768,  2268,   378,  6973,  1069,  6922,\n",
      "          7094,     9,   378,  2298,  9617, 12059,  1908,  8677,  1983,\n",
      "          3808,  3421,  1912,  5457,    19, 10881,  4656,    18, 16446,\n",
      "         19145,  5350, 29068,   378,  2130, 27261,  2854,  2366,     1,\n",
      "          2045,  2823, 12059,  1908,  3680, 13249,  2046,   378, 14474,\n",
      "          1010,  7276,  2045, 10212,     1,   378,  7627,  1955, 21370,\n",
      "         14939, 11114,    18,     1, 18985, 12426,  1013,   378,  4557,\n",
      "          3808,   378,  2130,  7772,  3808,  1912,  3532,  8736,  1017,\n",
      "           378,  3487,  4986, 22961,  4470,  3231,  3255,  5365,  8311,\n",
      "           378,  1908,  4462, 12430,  5919,  6953, 23107,  5365,  8311,\n",
      "          5563,   378,  3398,  2103,  1908, 12387, 15146, 13965,  1946,\n",
      "          1908,  3591,  2563,   378, 23306,  4063,  8216,  1043, 13869,\n",
      "          2600,    18,  5816,  2419,  1010,  3808,   378,  1912, 13848,\n",
      "          1028, 11161,   378,  1961,  6471,  3481,  2773,  7470, 19336,\n",
      "          7374,  7790,  5919,     6, 24554,     6, 28582, 15269,  2558,\n",
      "         14839, 19964, 25886,   378,  2813,  3328,     1,     1,   378,\n",
      "         18562,  2224,  4985,  3175,  1043, 25747,  1009,   411,  7094,\n",
      "          6596,     3]], dtype=int32)>,\n",
      " 'attention_mask': <tf.Tensor: shape=(1, 200), dtype=int32, numpy=\n",
      " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "in_tensor = prep_data_Camel(sample)\n",
    "print(in_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> this function will return both `input_ids` and `attention_mask` that we need for predicrion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Camel_predict(text):\n",
    "    test_tensor = prep_data_Camel(text)\n",
    "    probs = Camel_model_tuned.predict(test_tensor)\n",
    "    res = np.argmax(probs)\n",
    "    print(labels[res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class is :  Medical\n",
      "part of the news text :  تزامناً مع يوم العمال العالمي، الذي يحتفل به العالم في الأول من مايو من كل عام، نظم سوق الجبيل، يومي أمس  واليوم  إبريل الجاري، فحوصاً طبية مجانية للمستثمرين في السوق، والعمال، والمتسوقين، والزوار\n"
     ]
    }
   ],
   "source": [
    "sample_index = 150\n",
    "print(\"class is : \" , end = ' ')\n",
    "Camel_predict(X_val[sample_index])\n",
    "print(\"part of the news text : \" , end = ' ')\n",
    "print(X_val[sample_index][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AraBert : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install farasapy\n",
    "# !pip install pyarabic\n",
    "# !git clone https://github.com/aub-mind/arabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from py4j.java_gateway import JavaGateway\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "from transformers import TFBertForSequenceClassification , AutoModel\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "import arabert\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "\n",
    "from transformers import AutoConfig, BertForSequenceClassification, AutoTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-10-02 14:22:00,295 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "arabert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# arabert_model = AutoModel.from_pretrained(model_name)  ## BertForSequenceClassification\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "arabert_model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "## using automodel will give me an error because ` BertModel.forward() got an unexpected keyword argument 'labels' `  so i should use BertForSequenceClassification \n",
    "## which it's forward has labels in it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data =pd.DataFrame({\"text\":X_train,\"label\":y_train})\n",
    "val_data =pd.DataFrame({\"text\":X_val,\"label\":y_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = len(labels)\n",
    "task_name = 'classification'\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SingleSentenceClassificationProcessor(mode=task_name)\n",
    "val_dataset = SingleSentenceClassificationProcessor(mode=task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.add_examples(texts_or_text_and_labels=training_data['text'],labels=training_data['label'],overwrite_examples = True)\n",
    "val_dataset.add_examples(texts_or_text_and_labels=val_data['text'],labels=val_data['label'],overwrite_examples = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "train_features = train_dataset.get_features(tokenizer = arabert_tokenizer , max_length = max_length)\n",
    "val_features = val_dataset.get_features(tokenizer = arabert_tokenizer , max_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputFeatures(input_ids=[33, 1799, 59, 127, 20, 1892, 129, 2268, 16, 797, 12, 20, 1303, 20, 1187, 12, 6, 1512, 786, 20, 427, 12, 2, 1799, 455, 1688, 23970, 4538, 1512, 127, 18967, 803, 129, 6210, 20, 797, 12, 20, 3702, 12, 20, 5213, 130, 29, 419, 2556, 12, 322, 3909, 12, 5531, 5213, 12, 6488, 6, 20, 2229, 1779, 581, 290, 20, 797, 4, 29, 3730, 20, 1045, 12, 2, 2952, 5213, 29, 290, 2829, 916, 12, 289, 1188, 12, 1799, 48, 3426, 1538, 12, 20, 1512, 20, 5213, 20, 1361, 336, 1157, 5780, 12, 290, 20, 1255, 130, 2, 527, 8318, 797, 12, 20, 1303, 289, 20, 2829, 20, 359, 12, 29, 20, 1194, 20, 3327, 12, 289, 1188, 12, 1799, 713, 20, 2241, 4, 29, 20, 1941, 20, 1445, 12, 48, 2062, 20, 1512, 20, 5213, 20, 7053, 2, 3259, 29, 2701, 12, 9153, 797, 4, 650, 12, 290, 10, 797, 12, 1147, 20, 3331, 4, 2, 20, 1430, 16, 28, 20, 547, 12, 29, 20, 9252, 12, 130, 20, 1263, 2, 727, 1263, 797, 12, 20, 1303, 290, 455, 3259, 831, 5, 5962, 2, 20, 1512, 130, 20, 7736, 322, 3823, 727, 1037, 12, 837, 12, 6, 34], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=6)\n"
     ]
    }
   ],
   "source": [
    "print(train_features[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"./train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.do_train = True\n",
    "training_args.evaluate_during_training = True\n",
    "training_args.adam_epsilon = 1e-8\n",
    "training_args.learning_rate = 2e-5\n",
    "training_args.warmup_steps = 0\n",
    "training_args.per_gpu_train_batch_size = 16\n",
    "training_args.per_gpu_eval_batch_size = 16\n",
    "training_args.num_train_epochs= 5\n",
    "training_args.logging_steps = (len(train_features) - 1) // training_args.per_gpu_train_batch_size + 1\n",
    "training_args.save_steps = training_args.logging_steps \n",
    "training_args.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_utils.py\n",
    "def compute_metrics(p): #p should be of type EvalPrediction\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "\n",
    "  acc = accuracy_score(p.label_ids,preds)\n",
    "  return {\n",
    "      'accuracy': acc\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=arabert_model,\n",
    "                  args = training_args,\n",
    "                  train_dataset = train_features,\n",
    "                  eval_dataset = val_features ,\n",
    "                compute_metrics = compute_metrics\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output exceeds the size limit. Open the full output data in a text editor\n",
      "***** Running training *****\n",
      "Num examples = 31850\n",
      "Num Epochs = 5\n",
      "Instantaneous batch size per device = 8\n",
      "Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "Gradient Accumulation steps = 1\n",
      "Total optimization steps = 11946\n",
      "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…\n",
      "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=90.0, style=ProgressStyle(description_wid…\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 31850\n",
      "INFO:transformers.trainer:  Batch size = 16\n",
      "{\"loss\": 0.3570140757908424, \"learning_rate\": 1.6000000000000003e-05, \"epoch\": 1.0, \"step\": 90}\n",
      "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=23.0, style=ProgressStyle(description_wi…\n",
      "INFO:transformers.trainer:Saving model checkpoint to ./train/checkpoint-90\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./train/checkpoint-90/config.json\n",
      "    accuracy                           0.92   \n",
      "{\"eval_loss\": 0.23239801791699036 , \"eval_accuracy\": 0.9194444444444444, \"epoch\": 1.0, \"step\": 90}\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./train/checkpoint-90/pytorch_model.bin\n",
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=90.0, style=ProgressStyle(description_wid…\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 31850\n",
      "INFO:transformers.trainer:  Batch size = 16\n",
      "{\"loss\": 0.15852821198188596, \"learning_rate\": 1.2e-05, \"epoch\": 2.0, \"step\": 180}\n",
      "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=23.0, style=ProgressStyle(description_wi…\n",
      "INFO:transformers.trainer:Saving model checkpoint to ./train/checkpoint-180\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./train/checkpoint-180/config.json\n",
      "    accuracy                           0.91 \n",
      "{\"eval_loss\": 0.27613088439988054, \"eval_accuracy\": 0.9138888888888889, \"epoch\": 2.0, \"step\": 180}\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./train/checkpoint-180/pytorch_model.bin\n",
      "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=90.0, style=ProgressStyle(description_wid…\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 31850\n",
      "INFO:transformers.trainer:  Batch size = 16\n",
      "{\"loss\": 0.07192943679789703, \"learning_rate\": 8.000000000000001e-06, \"epoch\": 3.0, \"step\": 270}\n",
      "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=23.0, style=ProgressStyle(description_wi…\n",
      "INFO:transformers.trainer:Saving model checkpoint to ./train/checkpoint-270\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./train/checkpoint-270/config.json\n",
      "    accuracy                           0.91       \n",
      "{\"eval_loss\": 0.3215774331727754, \"eval_accuracy\": 0.9138888888888889, \"epoch\": 3.0, \"step\": 270}\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./train/checkpoint-270/pytorch_model.bin\n",
      "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=90.0, style=ProgressStyle(description_wid…\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 31850\n",
      "INFO:transformers.trainer:  Batch size = 16\n",
      "{\"loss\": 0.042770601871112986, \"learning_rate\": 4.000000000000001e-06, \"epoch\": 4.0, \"step\": 360}\n",
      "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=23.0, style=ProgressStyle(description_wi…\n",
      "INFO:transformers.trainer:Saving model checkpoint to ./train/checkpoint-360\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./train/checkpoint-360/config.json\n",
      "    accuracy                           0.92       \n",
      "{\"eval_loss\": 0.3710199777522813, \"eval_accuracy\": 0.9222222222222223, \"epoch\": 4.0, \"step\": 360}\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./train/checkpoint-360/pytorch_model.bin\n",
      "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=90.0, style=ProgressStyle(description_wid…\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 31850\n",
      "INFO:transformers.trainer:  Batch size = 16\n",
      "{\"loss\": 0.020281719416379927, \"learning_rate\": 0.0, \"epoch\": 5.0, \"step\": 450}\n",
      "HBox(children=(FloatProgress(value=0.0, description='Evaluation', max=23.0, style=ProgressStyle(description_wi…\n",
      "INFO:transformers.trainer:Saving model checkpoint to ./train/checkpoint-450\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./train/checkpoint-450/config.json\n",
      "    accuracy                           0.92           \n",
      "{\"eval_loss\": 0.38095345477695053, \"eval_accuracy\": 0.9222222222222223, \"epoch\": 5.0, \"step\": 450}\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.0036168249789625406},\n",
      "  {'label': 'LABEL_1', 'score': 0.0007229067268781364},\n",
      "  {'label': 'LABEL_2', 'score': 0.9777277708053589},  \n",
      "  {'label': 'LABEL_3', 'score': 0.0014909171732142568},\n",
      "  {'label': 'LABEL_4', 'score': 0.0009031872032210231},\n",
      "  {'label': 'LABEL_5', 'score': 0.014563223347067833},\n",
      "  {'label': 'LABEL_6', 'score': 0.0009752426412887871}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "pipe = TextClassificationPipeline(model=arabert_model, tokenizer=arabert_tokenizer, return_all_scores=True)\n",
    "pipe(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(model=arabert_model, tokenizer=arabert_tokenizer, return_all_scores=False)\n",
    "res = pipe(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(res[0]['label'])\n",
    "print(res[0]['label'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arabert_predict(text):\n",
    "    res = pipe(text)\n",
    "    int_res = int(res[0]['label'][-1]) # will return only the label\n",
    "    print(labels[int_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class is :  Medical\n",
      "part of the news text :  تزامناً مع يوم العمال العالمي، الذي يحتفل به العالم في الأول من مايو من كل عام، نظم سوق الجبيل، يومي أمس  واليوم  إبريل الجاري، فحوصاً طبية مجانية للمستثمرين في السوق، والعمال، والمتسوقين، والزوار\n"
     ]
    }
   ],
   "source": [
    "sample_index = 150\n",
    "print(\"class is : \" , end = ' ')\n",
    "arabert_predict(X_val[sample_index])\n",
    "print(\"part of the news text : \" , end = ' ')\n",
    "print(X_val[sample_index][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using : BERT multilingual base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████|972k/972k [00:00<00:00, 2.15MB/s]\n",
      "Downloading: 100%|██████████|29.0/29.0 [00:00<00:00, 887B/s]\n",
      "Downloading: 100%|██████████|625/625 [00:00<00:00, 15.8kB/s]\n",
      "Downloading: 100%|██████████|1.01G/1.01G [00:41<00:00, 17.0MB/s]\n",
      "\n",
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "multilingual_model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "text = X_train[0]\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = multilingual_model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  177853440 \n",
      "=================================================================\n",
      "Total params: 177,853,440\n",
      "Trainable params: 177,853,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "multilingual_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31850, 200)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 200\n",
    "num_samples = len(X_train)\n",
    "\n",
    "# initialize empty zero arrays\n",
    "Xids = np.zeros((num_samples, seq_len))\n",
    "Xmask = np.zeros((num_samples, seq_len))\n",
    "\n",
    "# check shape\n",
    "Xids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13650, 200)\n"
     ]
    }
   ],
   "source": [
    "num_samples_val = len(X_val)\n",
    "\n",
    "# initialize empty zero arrays\n",
    "Xids_val = np.zeros((num_samples_val, seq_len))\n",
    "Xmask_val = np.zeros((num_samples_val, seq_len))\n",
    "\n",
    "# check shape\n",
    "Xids_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, phrase in enumerate(X_train):\n",
    "    tokens = tokenizer.encode_plus(phrase, max_length=seq_len, truncation=True,\n",
    "                                   padding='max_length', add_special_tokens=True,\n",
    "                                   return_tensors='tf')\n",
    "    # assign tokenized outputs to respective rows in numpy arrays\n",
    "    Xids[i, :] = tokens['input_ids']\n",
    "    Xmask[i, :] = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, phrase in enumerate(X_val):\n",
    "    tokens = tokenizer.encode_plus(phrase, max_length=seq_len, truncation=True,\n",
    "                                   padding='max_length', add_special_tokens=True,\n",
    "                                   return_tensors='tf')\n",
    "    # assign tokenized outputs to respective rows in numpy arrays\n",
    "    Xids_val[i, :] = tokens['input_ids']\n",
    "    Xmask_val[i, :] = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((Xids_val, Xmask_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(input_ids, masks, labels):\n",
    "    # we convert our three-item tuple into a two-item tuple where the input item is a dictionary\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "# then we use the dataset map method to apply this transformation\n",
    "train_dataset = train_dataset.map(map_func)\n",
    "val_dataset = val_dataset.map(map_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will split into batches of 16\n",
    "batch_size = 16\n",
    "\n",
    "# shuffle and batch - dropping any remaining samples that don't cleanly\n",
    "# fit into a batch of 16\n",
    "train_dataset = train_dataset.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = len(labels)\n",
    "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
    "\n",
    "\n",
    "embeddings = multilingual_model.bert(input_ids, attention_mask=mask)[1]  # access pooled activations with [1]\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(classes, activation='softmax', name='outputs')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_model_tuned = tf.keras.Model(inputs=[input_ids, mask], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          multiple             177853440   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         787456      bert[3][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 7)            7175        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 178,648,071\n",
      "Trainable params: 178,648,071\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "multilingual_model_tuned.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "multilingual_model_tuned.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1991/1991 [==============================] - 913s 570ms/step - loss:  0.537819 - accuracy: 0.7822 \n",
      "Epoch 2/5\n",
      "1991/1991 [==============================] - 811s 507ms/step - loss: 0.409763  - accuracy: 0.7909\n",
      "Epoch 3/5\n",
      "1991/1991 [==============================] - 811s 507ms/step - loss:  0.451422 - accuracy: 0.8109\n",
      "Epoch 4/5\n",
      "1991/1991 [==============================] - 811s 507ms/step - loss: 0.332129 - accuracy: 0.8742 \n",
      "Epoch 5/5\n",
      "1991/1991 [==============================] - 811s 507ms/step - loss: 0.366856  - accuracy: 0.8804\n"
     ]
    }
   ],
   "source": [
    "history = multilingual_model_tuned.fit(\n",
    "    train_dataset,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_multilingual(text):\n",
    "    # tokenize to get input IDs and attention mask tensors\n",
    "    tokens = tokenizer.encode_plus(text, max_length=seq_len,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_token_type_ids=False,\n",
    "                                   return_tensors='tf')\n",
    "    return {'input_ids': tf.cast(tokens['input_ids'], tf.int32),\n",
    "            'attention_mask': tf.cast(tokens['attention_mask'], tf.int32)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 200), dtype=int32, numpy=\n",
      "array([[   101,  65961,  59901,  16498,  88359,    118,    781,  22955,\n",
      "         10765,    781,  37423,  67596,    131,  28598,    771,    119,\n",
      "         14610,  15764,  59901,  50643,  20556,  10461,    752,  93800,\n",
      "         21251,  65961,  59901,  16498,  88359,  85549,    131,  26351,\n",
      "         16404,  59901, 107559,  62765,  10210,  71403,    761,  37548,\n",
      "         13154,  24979,  59901,  49170,  10673,    787,  34418,  54422,\n",
      "         10535, 106487,  20250,  10535,    752,  13121,  72197,  10289,\n",
      "           759,  43348,  59901,  24639,  40446,  11509, 106487,  10210,\n",
      "         59901, 111171,  93864,    752,  13623,    763,  25050,  10502,\n",
      "         12616,  11437,    752,  10723,  17652,  10502,  11417,    110,\n",
      "           752,  19137,  59901,  34783,  28241,  11693,  59901,  10765,\n",
      "         20132,  12611,  10289,    766,  56363,  14472,  23411,  57150,\n",
      "         10210,  23485,    120,  14500,  21679,  59901,  10700,  44528,\n",
      "           119,    791,  29336,  10658,  59901,  10700,  65870, 101293,\n",
      "           787,  22887,  67077,    752,  13121,    793,  40041,  63673,\n",
      "         27660,  24738,    788,  45099,  39053,  11870,  10863,  29735,\n",
      "         59901,  10765,  20132,  12611,  10289,  71403,  59901, 111171,\n",
      "         37548,  13154,    752,    791,  10461,  28307,  23838,  10388,\n",
      "         10863,  11349,  20719,    752,  56560,  11749,    788,  11091,\n",
      "         32195,    776,  20907,  10461,  16141,  20556,    119,    791,\n",
      "         22468,  41003,  16275,    787,  48649,  12497,  56644,  59901,\n",
      "         50643,  20556,  10461,    752,  37554,  34141,  23411,    752,\n",
      "         13121,  26348,  23411,  10210,  21251,  59901,  49170,  10673,\n",
      "           752,  33807,  10533, 104864,  10429,  21737,  24979,  17065,\n",
      "         65961,  59901,  16498,  88359,    752,  10289,  30401,    102]],\n",
      "      dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 200), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "sample = X_train[0] \n",
    "in_tensor = prep_data_multilingual(sample)\n",
    "print(in_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilingual_predict(text):\n",
    "    test_tensor = prep_data_multilingual(text)\n",
    "    probs = multilingual_model_tuned.predict(test_tensor)\n",
    "    res = np.argmax(probs)\n",
    "    print(labels[res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class is :  Medical\n",
      "part of the news text :  تزامناً مع يوم العمال العالمي، الذي يحتفل به العالم في الأول من مايو من كل عام، نظم سوق الجبيل، يومي أمس  واليوم  إبريل الجاري، فحوصاً طبية مجانية للمستثمرين في السوق، والعمال، والمتسوقين، والزوار\n"
     ]
    }
   ],
   "source": [
    "sample_index = 150\n",
    "print(\"class is : \" , end = ' ')\n",
    "multilingual_predict(X_val[sample_index])\n",
    "print(\"part of the news text : \" , end = ' ')\n",
    "print(X_val[sample_index][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " | Model                                | Epochs | Time  | train_Acc |\n",
    "  | ------------------------------------ | ------ | ------ | ----- |\n",
    "  | bag_of_words             | _  | 3 min  | 97.52%    |\n",
    "  | aravec            | _ | 4 min  | 97.03%   |\n",
    "  | LSTM        | 30 epochs  | 3h  | 95.4%   |\n",
    "  | CaMel       | 5 epochs | 1.2h  | 94%   |\n",
    "  | AraBert       | 5 epochs  | 2h | 92%   |\n",
    "  | BERT_Multilingual      | 5 epochs  | 1.5h | 88%   | |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `BERT` models can get us better results with more training "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f826061a63cad14e0ca8803dd2f754c9f5a32ee583de9c2a5f59bd7c7ae363c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
